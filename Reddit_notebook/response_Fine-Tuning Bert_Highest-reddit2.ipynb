{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"response_Fine-Tuning Bert_Highest-reddit2.ipynb","provenance":[{"file_id":"1FdZjlf-71sxhRiAzpixL6KhFr69JBLyW","timestamp":1621792346628},{"file_id":"1ftGKQ22ZBSIX_d6KEpUI7vRczYgtQPMF","timestamp":1620983839237},{"file_id":"https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb","timestamp":1619947106006}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OFOTiqrtNvyy"},"source":["# Install Transformers Library"]},{"cell_type":"code","metadata":{"id":"Vx_or9rpuJ-g","executionInfo":{"status":"ok","timestamp":1626713018018,"user_tz":420,"elapsed":5,"user":{"displayName":"abhishek ranjan","photoUrl":"","userId":"07093215491084358593"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sYuExU3YiR4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hkhc10wNrGt"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4giRzM7NtHJ"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","# from transformers import AutoModel, BertTokenizerFast\n","from transformers import RobertaTokenizer, RobertaModel\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKd-Tj3hOMsZ"},"source":["# Load Dataset"]},{"cell_type":"code","metadata":{"id":"cwJrQFQgN_BE"},"source":["df = pd.read_json('/content/drive/MyDrive/sarcasm-detection/Datasets/reddit/sarcasm_detection_shared_task_reddit_training.jsonl',lines=True)\n","df1 = pd.read_json('/content/drive/MyDrive/sarcasm-detection/Datasets/reddit/sarcasm_detection_shared_task_reddit_testing.jsonl',lines=True)\n","\n","df['labels'] = df['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1['labels'] = df1['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzk_I0RLTaSh"},"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","import re\n","import string\n","import numpy as np\n","np.random.seed(0)\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from tqdm.auto import tqdm\n","!pip install demoji\n","!pip install contractions\n","import demoji\n","import contractions\n","\n","demoji.download_codes()\n","nltk.download('stopwords') \n","# nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Je5yW5oDTaFm"},"source":["def preprocess_text(text):\n","    # Tokenise words while ignoring punctuation(https://www.nltk.org/_modules/nltk/tokenize/regexp.html)\n","    text = re.sub(r\"@\",'', text)\n","    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n","    text = re.sub(r\"USER\",'@USER', text)\n","    # text = re.sub(r\"USER\",'', text)\n","    text = re.sub(r'<URL>','',text)\n","\n","    text = demoji.replace_with_desc(text)\n","    text = re.sub(r':','',text)\n","    text = contractions.fix(text)\n","    # text = re.sub(r'\\.+','',text)\n","    return text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qQHiUqcTaCi"},"source":["def fun(A,l):\n","  if len(A)<=l:\n","    return A\n","  else:\n","    return  A[-l::1]\n","    # return A[-1:-l-1:-1]\n","def fun1(A):\n","  return ''.join(A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3JEOeVFTaAi"},"source":["l=2\n","\n","# df['tweets']= df['response'] +'reply-'+ df['context'].apply(fun,args=[l]).apply(fun1)\n","# df1['tweets']=df1['response'] +'reply-'+ df1['context'].apply(fun,args=[l]).apply(fun1)\n","\n","df['tweets']= 'reply-'+ df['response']# + ' context- ' + df['context'].apply(fun,args=[l]).apply(fun1)\n","df1['tweets']= 'reply-'+ df1['response']# +  ' context- ' +  df1['context'].apply(fun,args=[l]).apply(fun1)\n","\n","\n","# df['tweets']= df['context'].apply(fun,args=[l]).apply(fun1) + df['response']\n","# df1['tweets']= df1['context'].apply(fun,args=[l]).apply(fun1) + df1['response']\n","\n","df['tweets']= df['tweets'].apply(preprocess_text) \n","df1['tweets']=df1['tweets'].apply(preprocess_text)\n","# df['tweets']=df['tweets']+'reply-'+ df['response'].apply(preprocess_text)\n","# df1['tweets']=df1['tweets']+'reply-'+ df1['response'].apply(preprocess_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iDOTEjvTiYm"},"source":["# check class distribution\n","df['labels'].value_counts(normalize = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHniIqA8TiVS"},"source":["df.loc[2628,'tweets']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VvXO-bATiTZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mI175LEeTZ-a"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7WBye79ZKoO"},"source":["# df[\"tweets\"]=df['response']\n","# df1[\"tweets\"]=df1['response']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzPPOrVQWiW5"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"676DPU1BOPdp"},"source":["# check class distribution\n","df['labels'].value_counts(normalize = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MKfWnApvOoE7"},"source":["# Split train dataset into train, validation and test sets"]},{"cell_type":"code","metadata":{"id":"mfhSPF5jOWb7"},"source":["\n","X_s = df['tweets'].values\n","y_s = df['labels'].values\n","\n","Xt_s = df1['tweets'].values\n","yt_s = df1['labels'].values\n","\n","train_text, temp_text, train_labels, temp_labels = X_s,Xt_s,y_s,yt_s\n","\n","# we will use temp_text and temp_labels to create validation and test set\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n","                                                                random_state=2018, \n","                                                                test_size=0.5, \n","                                                                stratify=temp_labels)\n","val_text, test_text, val_labels, test_labels=temp_text, temp_text, temp_labels, temp_labels\n","\n","# X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.2, random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1jEFb39cCjk"},"source":["# pip install -U sentence-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7hsdLoCO7uB"},"source":["# Import BERT Model and BERT Tokenizer"]},{"cell_type":"code","metadata":{"id":"S1kY3gZjO2RE"},"source":["from transformers import RobertaTokenizer, TFRobertaModel\n","bert = RobertaModel.from_pretrained('roberta-large')\n","\n","# Load the BERT tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zOKeOMeO-DT"},"source":["# sample data\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n","# encode text\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAH73n39PHLw"},"source":["# output\n","print(sent_id)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8wIYaWI_Prg8"},"source":["# Tokenization"]},{"cell_type":"code","metadata":{"id":"yKwbpeN_PMiu"},"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_text]\n","pd.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXcswEIRPvGe"},"source":["max_seq_len = 120"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tk5S7DWaP2t6"},"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wsm8bkRZQTw9"},"source":["# Convert Integer Sequences to Tensors"]},{"cell_type":"code","metadata":{"id":"QR-lXwmzQPd6"},"source":["# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ov1cOBlcRLuk"},"source":["# Create DataLoaders"]},{"cell_type":"code","metadata":{"id":"qUy9JKFYQYLp"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2HZc5ZYRV28"},"source":["# Freeze BERT Parameters"]},{"cell_type":"code","metadata":{"id":"wHZ0MC00RQA_"},"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = True\n","# print(bert)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7ahGBUWRi3X"},"source":["# Define Model Architecture"]},{"cell_type":"code","metadata":{"id":"b3iEtGyYRd0A"},"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","      \n","      super(BERT_Arch, self).__init__()\n","\n","      self.bert = bert \n","      \n","      # dropout layer\n","      self.dropout = nn.Dropout(0.1)\n","      \n","      # relu activation function\n","      self.relu =  nn.ReLU()\n","\n","      # dense layer 1\n","      self.fc1 = nn.Linear(1024,512)\n","      \n","      # dense layer 2 (Output layer)\n","      self.fc2 = nn.Linear(512,2)\n","\n","      #softmax activation function\n","      self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","      #pass the inputs to the model  \n","      _, cls_hs = self.bert(sent_id,mask, return_dict=False)\n","      \n","      x = self.fc1(cls_hs)\n","\n","      x = self.relu(x)\n","\n","      x = self.dropout(x)\n","\n","      # output layer\n","      x = self.fc2(x)\n","      \n","      # apply softmax activation\n","      x = self.softmax(x)\n","\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBAJJVuJRliv"},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taXS0IilRn9J"},"source":["# # optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# # define the optimizer\n","# decayRate = 0.99\n","optimizer = AdamW(model.parameters(), lr = 2e-5)\n","# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDFcA6vUERxD"},"source":["# from scheduler import CyclicCosineDecayLR\n","# import torch.optim as optim\n","# # optimizer = optim.SGD(model.parameters(), lr=2e-3)\n","# my_lr_scheduler = CyclicCosineDecayLR(optimizer, \n","#                                 init_decay_epochs=3,\n","#                                 min_decay_lr=5e-6,\n","#                                 restart_interval=6,\n","#                                 restart_interval_multiplier=1.2,\n","#                                 restart_lr=5e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9CDpoMQR_rK"},"source":["# Find Class Weights"]},{"cell_type":"code","metadata":{"id":"izY5xH5eR7Ur"},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1WvfY2vSGKi"},"source":["# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2e2S4dA2NFo1"},"source":["from tqdm.auto import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"My4CA0qaShLq"},"source":["# Fine-Tune BERT"]},{"cell_type":"code","metadata":{"id":"rskLk8R_SahS"},"source":["# function to train the model\n","def train():\n","  \n","  model.train()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in tqdm(enumerate(train_dataloader),total=4400//batch_size+1):\n","    # progress update after every 50 batches.\n","    # if step % 50 == 0 and not step == 0:\n","    #   print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n"," \n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","    # my_lr_scheduler.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGXovFDlSxB5"},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  # for step,batch in enumerate(val_dataloader):\n","  for step,batch in tqdm(enumerate(val_dataloader),total=1800//batch_size+1):\n","    # Progress update every 50 batches.\n","    # if step % 50 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","      # elapsed = format_time(time.time() - t0)\n","            \n","      # Report progress.\n","      # print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CfEiMqAGHQW"},"source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9KZEgxRRTLXG"},"source":["# Start Model Training"]},{"cell_type":"code","metadata":{"id":"k1USGTntS3TS"},"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","epochs=30\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    for param_group in optimizer.param_groups:\n","        print('learning_rate- ',param_group['lr'])\n","    #train model\n","    train_loss,_ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    # if valid_loss < best_valid_loss:\n","    best_valid_loss = valid_loss\n","    torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    pred=[]\n","    with torch.no_grad():\n","      for i in tqdm(range(0,90)):\n","        preds = model(test_seq[i*20:i*20+20].to(device), test_mask[i*20:i*20+20].to(device))\n","        preds = preds.detach().cpu().numpy()\n","        pred.append(preds)\n","    pred1=np.array(pred)\n","    pred2=pred1.reshape((1800,2))\n","    p=pd.DataFrame(pred2)\n","    p.to_csv('71_Rresponse.csv')\n","    pred3 = np.argmax(pred2, axis = 1)\n","    print(classification_report(test_y, pred3))\n","    print(accuracy_score(test_y, pred3))\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_yrhUc9kTI5a"},"source":["# Load Saved Model"]},{"cell_type":"code","metadata":{"id":"OacxUyizS8d1"},"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4SVftkkTZXA"},"source":["# Get Predictions for Test Data"]},{"cell_type":"code","metadata":{"id":"ipPMhvw5R9ia"},"source":["pred=[]\n","with torch.no_grad():\n","  for i in range(0,90):\n","    preds = model(test_seq[i*20:i*20+20].to(device), test_mask[i*20:i*20+20].to(device))\n","    preds = preds.detach().cpu().numpy()\n","    pred.append(preds)\n","# print(np.array(pred).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4llNN4ESBZx"},"source":["pred1=np.array(pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfPzJqI4SBWP"},"source":["pred2=pred1.reshape((1800,2))\n","# pred2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9TocyBuufHp"},"source":["p=pd.DataFrame(pred2)\n","p.to_csv('71_Rresponse.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4_WPoDfSBUg"},"source":["pred3 = np.argmax(pred2, axis = 1)\n","# pred3 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKhMDE14SQuu"},"source":["print(classification_report(test_y, pred3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"To_49cNySQrO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpX1uTwjUPY6"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BY2P7H1W2Wo4"},"source":[""],"execution_count":null,"outputs":[]}]}