{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"robertra_29_06_large_context.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OFOTiqrtNvyy"},"source":["# Install Transformers Library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7sYuExU3YiR4","executionInfo":{"status":"ok","timestamp":1626431235814,"user_tz":-330,"elapsed":573,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"66c3e1ea-a509-4fe1-af21-fd0e1d75389a"},"source":["import random\n","seed=0\n","random.seed(seed)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hkhc10wNrGt","executionInfo":{"status":"ok","timestamp":1626430910970,"user_tz":-330,"elapsed":1763,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"06c5ae6e-56b5-4c1e-891b-bf19d6ae3ac0"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4giRzM7NtHJ"},"source":["# import numpy as np\n","import pandas as pd\n","import torch\n","torch.manual_seed(0)\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","# from transformers import AutoModel, BertTokenizerFast\n","from transformers import RobertaTokenizer, RobertaModel\n","# specify GPU\n","device = torch.device(\"cuda\")\n","from sklearn.utils import shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNiXc3ds2Hbr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"cwJrQFQgN_BE","executionInfo":{"status":"ok","timestamp":1626431116540,"user_tz":-330,"elapsed":2726,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"5ea741fd-73b0-4c1c-f9fc-0bf1836d19de"},"source":["df = pd.read_json('/content/drive/MyDrive/sarcasm detection/Datasets/reddit/sarcasm_detection_shared_task_reddit_training.jsonl',lines=True)\n","df1 = pd.read_json('/content/drive/MyDrive/sarcasm detection/Datasets/reddit/sarcasm_detection_shared_task_reddit_testing.jsonl',lines=True)\n","df['labels'] = df['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1['labels'] = df1['label'].apply(lambda x: ['SARCASM', 'NOT_SARCASM'].index(x))\n","df1.tail()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>context</th>\n","      <th>response</th>\n","      <th>id</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1795</th>\n","      <td>SARCASM</td>\n","      <td>[Spoiler Cristiane Justino vs. Amanda Nunes, H...</td>\n","      <td>she will probably beat him too. she can fight ...</td>\n","      <td>reddit_1796</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1796</th>\n","      <td>NOT_SARCASM</td>\n","      <td>[Treyarch makes the most unbalanced game ever....</td>\n","      <td>No, but then the game really glitch(ed) out fo...</td>\n","      <td>reddit_1797</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1797</th>\n","      <td>SARCASM</td>\n","      <td>[First Official Image from \"Zombieland: Double...</td>\n","      <td>&gt; Zombieland ~~2:~~ Double Tap They changed it...</td>\n","      <td>reddit_1798</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1798</th>\n","      <td>SARCASM</td>\n","      <td>[Its time to ban /r/The_Donald: Calling out sp...</td>\n","      <td>We're not *supporting racists* and prospective...</td>\n","      <td>reddit_1799</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1799</th>\n","      <td>SARCASM</td>\n","      <td>[59 Alabama ministers sign a letter saying Roy...</td>\n","      <td>Is she old enough to have Facebook</td>\n","      <td>reddit_1800</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            label  ... labels\n","1795      SARCASM  ...      0\n","1796  NOT_SARCASM  ...      1\n","1797      SARCASM  ...      0\n","1798      SARCASM  ...      0\n","1799      SARCASM  ...      0\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"IqQTsKO5NPlW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"654bRbB5ofFh"},"source":["df=shuffle(df,random_state=seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DIY6yOMr8sdX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626431442997,"user_tz":-330,"elapsed":10033,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"3308b644-c5f2-41d2-ee6e-3a45f910c559"},"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","import re\n","import string\n","import numpy as np\n","np.random.seed(0)\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from tqdm.auto import tqdm\n","!pip install demoji\n","!pip install contractions\n","import demoji\n","import contractions\n","\n","demoji.download_codes()\n","nltk.download('stopwords') \n","# nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting demoji\n","  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n","Installing collected packages: colorama, demoji\n","Successfully installed colorama-0.4.4 demoji-0.4.0\n","Collecting contractions\n","  Downloading https://files.pythonhosted.org/packages/93/f4/0ec4a458e4368cc3be2c799411ecf0bc961930e566dadb9624563821b3a6/contractions-0.0.52-py2.py3-none-any.whl\n","Collecting textsearch>=0.0.21\n","  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n","Collecting pyahocorasick\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n","\u001b[K     |████████████████████████████████| 327kB 9.2MB/s \n","\u001b[?25hCollecting anyascii\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n","\u001b[K     |████████████████████████████████| 286kB 40.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85400 sha256=548e63953379f5b81037fbd573abb277ba6d76b49ab16622a4f36ae643abd5f6\n","  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n","Successfully built pyahocorasick\n","Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n","Downloading emoji data ...\n","... OK (Got response in 0.17 seconds)\n","Writing emoji data to /root/.demoji/codes.json ...\n","... OK\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"Z5qBvDp14S4z"},"source":["def preprocess_text(text):\n","    # Tokenise words while ignoring punctuation(https://www.nltk.org/_modules/nltk/tokenize/regexp.html)\n","    text = re.sub(r\"@\",'', text)\n","    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n","    text = re.sub(r\"USER\",'@USER', text)\n","    # text = re.sub(r\"USER\",'', text)\n","    text = re.sub(r'<URL>','',text)\n","\n","    text = demoji.replace_with_desc(text)\n","    text = re.sub(r':','',text)\n","    text = contractions.fix(text)\n","    # text = re.sub(r'\\.+','',text)\n","    return text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nj-zpWUjt0IB"},"source":["def fun(A,l):\n","  if len(A)<=l:\n","    return A\n","  else:\n","    return  A[-l::1]\n","    # return A[-1:-l-1:-1]\n","def fun1(A):\n","  return ''.join(A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3DFJt513x7X"},"source":["l=3\n","\n","# df['tweets']= df['response'] +'reply-'+ df['context'].apply(fun,args=[l]).apply(fun1)\n","# df1['tweets']=df1['response'] +'reply-'+ df1['context'].apply(fun,args=[l]).apply(fun1)\n","\n","df['tweets']= 'reply-'+ df['response'] + ' context- ' + df['context'].apply(fun,args=[l]).apply(fun1)\n","df1['tweets']= 'reply-'+ df1['response'] +  ' context- ' +  df1['context'].apply(fun,args=[l]).apply(fun1)\n","\n","\n","# df['tweets']= df['context'].apply(fun,args=[l]).apply(fun1) + df['response']\n","# df1['tweets']= df1['context'].apply(fun,args=[l]).apply(fun1) + df1['response']\n","\n","df['tweets']= df['tweets'].apply(preprocess_text) \n","df1['tweets']=df1['tweets'].apply(preprocess_text)\n","# df['tweets']=df['tweets']+'reply-'+ df['response'].apply(preprocess_text)\n","# df1['tweets']=df1['tweets']+'reply-'+ df1['response'].apply(preprocess_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"676DPU1BOPdp","executionInfo":{"status":"ok","timestamp":1626431685913,"user_tz":-330,"elapsed":22,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"6a22a005-173b-4ac5-9b5b-d69eb2264cce"},"source":["# check class distribution\n","df['labels'].value_counts(normalize = True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    0.5\n","0    0.5\n","Name: labels, dtype: float64"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"mjLmL_LFtVjo","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1626431741184,"user_tz":-330,"elapsed":422,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"80c25c2e-4f13-4f65-cfaf-f7a2fef106c4"},"source":["df.loc[2628,'tweets']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"reply-The joke works if you say the other is the Nationals, too. context- Now actual senators want Papelbon out of Washington, hot sports takes on C-SPANThe Nats and Congress... one's a dysfunctional, infighting body, full of disappointment and sadness that just picked a leader no one has faith in... and the other is Congress.\""]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"MKfWnApvOoE7"},"source":["# Split train dataset into train, validation and test sets"]},{"cell_type":"code","metadata":{"id":"mfhSPF5jOWb7"},"source":["\n","X_s = df['tweets'].values\n","y_s = df['labels'].values\n","\n","Xt_s = df1['tweets'].values\n","yt_s = df1['labels'].values\n","\n","train_text, temp_text, train_labels, temp_labels = X_s,Xt_s,y_s,yt_s\n","\n","# we will use temp_text and temp_labels to create validation and test set\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n","                                                                random_state=0, \n","                                                                test_size=0.5, \n","                                                                stratify=temp_labels)\n","val_text, test_text, val_labels, test_labels=temp_text, temp_text, temp_labels, temp_labels\n","\n","# X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.2, random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1jEFb39cCjk"},"source":["# pip install -U sentence-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7hsdLoCO7uB"},"source":["# Import BERT Model and BERT Tokenizer\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1kY3gZjO2RE","executionInfo":{"status":"ok","timestamp":1626431966490,"user_tz":-330,"elapsed":6780,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"14194aef-a0be-4751-b830-6ac3fedd19a4"},"source":["# import BERT-base pretrained model\n","# bert = AutoModel.from_pretrained('bert-base-uncased')\n","\n","# # Load the BERT tokenizer\n","# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","# import BERT-base pretrained model\n","\n","\n","# from transformers import RobertaTokenizer, TFRobertaModel\n","# bert = RobertaModel.from_pretrained('roberta-base')\n","\n","# # Load the BERT tokenizer\n","# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","from transformers import RobertaTokenizer, TFRobertaModel\n","bert = RobertaModel.from_pretrained('roberta-large')\n","\n","# Load the BERT tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_zOKeOMeO-DT"},"source":["# sample data\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n","# encode text\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAH73n39PHLw","executionInfo":{"status":"ok","timestamp":1626431966494,"user_tz":-330,"elapsed":22,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"a32dcf4a-5cdf-44bc-dbb0-263bea6f704a"},"source":["# output\n","print(sent_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input_ids': [[0, 9226, 16, 10, 741, 2399, 1421, 35950, 2, 1, 1, 1], [0, 1694, 40, 2051, 12, 90, 4438, 10, 741, 2399, 1421, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8wIYaWI_Prg8"},"source":["# Tokenization"]},{"cell_type":"code","metadata":{"id":"yKwbpeN_PMiu"},"source":["# # get length of all the messages in the train set\n","# seq_len = [len(i.split()) for i in train_text]\n","# for i in train_text:\n","#   print(i)\n","# pd.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXcswEIRPvGe"},"source":["max_seq_len = 100\n","# max_seq_len = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGcvb0cV5irf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626431922676,"user_tz":-330,"elapsed":430,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"cf364064-c258-4c86-afe2-0b691f413824"},"source":["total=0\n","for i in range(4400):\n","  total+=len(df['tweets'][i].split(' '))\n","total/4400"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["45.25022727272727"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"tk5S7DWaP2t6"},"source":["# tokenize and encode sequences in the training set\n","\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    padding=True,\n","    truncation=True,    \n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    padding=True,\n","    truncation=True,\n","    return_token_type_ids=False\n","    \n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    padding=True,\n","    truncation=True,\n","    return_token_type_ids=False\n","    \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wsm8bkRZQTw9"},"source":["# Convert Integer Sequences to Tensors"]},{"cell_type":"code","metadata":{"id":"QR-lXwmzQPd6"},"source":["# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ov1cOBlcRLuk"},"source":["# Create DataLoaders"]},{"cell_type":"code","metadata":{"id":"qUy9JKFYQYLp"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2HZc5ZYRV28"},"source":["# Freeze BERT Parameters"]},{"cell_type":"code","metadata":{"id":"wHZ0MC00RQA_"},"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = True\n","# print(bert)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7ahGBUWRi3X"},"source":["# Define Model Architecture"]},{"cell_type":"code","metadata":{"id":"b3iEtGyYRd0A"},"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","      \n","      super(BERT_Arch, self).__init__()\n","\n","      self.bert = bert \n","      \n","      # dropout layer\n","      self.dropout = nn.Dropout(0.1)\n","      \n","      # relu activation function\n","      self.relu =  nn.ReLU()\n","\n","      # dense layer 1\n","      # self.fc1 = nn.Linear(768,256)\n","      self.fc1 = nn.Linear(1024,256)\n","      # dense layer 2 (Output layer)\n","      self.fc2 = nn.Linear(256,2)\n","\n","      #softmax activation function\n","      self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","      #pass the inputs to the model  \n","      _, cls_hs = self.bert(sent_id,mask, return_dict=False)\n","      # print(\"dfghjjhfchgbhlvmb jfbvfddvfgbhnjhgvcvtynubtrvcebnuexcvbnbtvrcedcvbnjbnjn11111 \",cls_hs.shape)\n","      x = self.fc1(cls_hs)\n","      # print(\"dfghjjhfchgbhlvmb jfbvfddvfgbhnjhgvcvtynubtrvcebnuexcvbnbtvrcedcvbnjbnjn22222 \",x.shape)\n","      x = self.relu(x)\n","      # print(\"dfghjjhfchgbhlvmb jfbvfddvfgbhnjhgvcvtynubtrvcebnuexcvbnbtvrcedcvbnjbnjn22222 \",x.shape)\n","      x = self.dropout(x)\n","      # print(\"dfghjjhfchgbhlvmb jfbvfddvfgbhnjhgvcvtynubtrvcebnuexcvbnbtvrcedcvbnjbnjn22222 \",x.shape)\n","      # output layer\n","      x = self.fc2(x)\n","      # print(\"dfghjjhfchgbhlvmb jfbvfddvfgbhnjhgvcvtynubtrvcebnuexcvbnbtvrcedcvbnjbnjn22222 \",x.shape)\n","      # apply softmax activation\n","      x = self.softmax(x)\n","      # print(\"dfghjjhfchgbhlvmb jfbvfddvfgbhnjhgvcvtynubtrvcebnuexcvbnbtvrcedcvbnjbnjn22222 \",x.shape)\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBAJJVuJRliv"},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taXS0IilRn9J"},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 2e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9CDpoMQR_rK"},"source":["# Find Class Weights"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"izY5xH5eR7Ur","executionInfo":{"status":"ok","timestamp":1626432039624,"user_tz":-330,"elapsed":10,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"a67b85bb-902b-463e-a5c4-dc074e4ce172"},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1. 1.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r1WvfY2vSGKi"},"source":["# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"My4CA0qaShLq"},"source":["# Fine-Tune BERT"]},{"cell_type":"code","metadata":{"id":"rskLk8R_SahS"},"source":["# function to train the model\n","def train():\n","  \n","  model.train()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    \n","    # progress update after every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n"," \n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGXovFDlSxB5"},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    \n","    # Progress update every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","      # elapsed = format_time(time.time() - t0)\n","            \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHQlio6yM_QH","executionInfo":{"status":"ok","timestamp":1626432043939,"user_tz":-330,"elapsed":712,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"a9ecdbfc-7c77-449f-c0f6-3685b4314d6d"},"source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri Jul 16 10:40:41 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    25W /  70W |   2436MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9KZEgxRRTLXG"},"source":["# Start Model Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1USGTntS3TS","executionInfo":{"status":"ok","timestamp":1626432337557,"user_tz":-330,"elapsed":282950,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"029ab330-bc34-4d03-94c4-19bfee060bc5"},"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","epochs=1\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss,_ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    # if valid_loss < best_valid_loss:\n","    best_valid_loss = valid_loss\n","    torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    # get predictions for test data\n","    !nvidia-smi\n","    # with torch.no_grad():-\n","    #   preds = model(test_seq.to(device), test_mask.to(device))\n","    #   preds = preds.detach().cpu().numpy()\n","    # preds = np.argmax(preds, axis = 1)\n","    # print(classification_report(test_y, preds))\n","    # print(accuracy_score(test_y, preds))\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 1\n","  Batch    50  of    138.\n","  Batch   100  of    138.\n","\n","Evaluating...\n","  Batch    50  of     57.\n","Fri Jul 16 10:45:35 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   72C    P0    30W /  70W |  13718MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","\n","Training Loss: 0.694\n","Validation Loss: 0.693\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_yrhUc9kTI5a"},"source":["# Load Saved Model"]},{"cell_type":"code","metadata":{"id":"OacxUyizS8d1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626432338259,"user_tz":-330,"elapsed":705,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"bdf480fb-a7d8-4769-ddbb-730f2cb31211"},"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"x4SVftkkTZXA"},"source":["# Get Predictions for Test Data"]},{"cell_type":"code","metadata":{"id":"NZl0SZmFTRQA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626432373152,"user_tz":-330,"elapsed":34897,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"e0fef261-207a-46dd-b5e3-1de8f97472cb"},"source":["pred=[]\n","with torch.no_grad():\n","  for i in range(0,90):\n","    preds = model(test_seq[i*20:i*20+20].to(device), test_mask[i*20:i*20+20].to(device))\n","    preds = preds.detach().cpu().numpy()\n","    pred.append(preds)\n","print(np.array(pred).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(90, 20, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WfHQYSOAgLyT"},"source":["pred1=np.array(pred)\n","# pred1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmSzWK63gL5z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626432472268,"user_tz":-330,"elapsed":529,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"87a9e1d7-61f2-417b-95c4-d1e9f2b61efc"},"source":["pred2=pred1.reshape((1800,2))\n","pred2.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1800, 2)"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"nQ_1m2WhgL7k"},"source":["p=pd.DataFrame(pred2)\n","p.to_csv('context_2.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8su6xM8gSNJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626432505686,"user_tz":-330,"elapsed":3,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"677474e1-d7e1-4f08-dcaf-78ce4877e89d"},"source":["pred3 = np.argmax(pred2, axis = 1)\n","pred3 "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"ZMw2NRnngSQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626432508814,"user_tz":-330,"elapsed":700,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"8f3cca89-354e-4dda-dafa-c6aaed9d59a1"},"source":["print(classification_report(test_y, pred3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.50      1.00      0.67       900\n","           1       0.00      0.00      0.00       900\n","\n","    accuracy                           0.50      1800\n","   macro avg       0.25      0.50      0.33      1800\n","weighted avg       0.25      0.50      0.33      1800\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5K4dl1AQgShx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626432514313,"user_tz":-330,"elapsed":450,"user":{"displayName":"SWAPNIL KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsLZIPPZoD8cm6pYXnJ-E5s3jPYct0P1FaVpMB1w=s64","userId":"13060826743241505268"}},"outputId":"a83f0273-3614-40e9-82b3-2989fd9ae07d"},"source":["# confusion matrix\n","# print(pred)\n","pd.crosstab(test_y, pred3)\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","print(accuracy_score(test_y, pred3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W14537b4ubb6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0psIEgeubdz"},"source":["# brcc=pd.read_csv('context_2.csv')\n","# brcc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eyDGkYuubhK"},"source":["# brc=pd.read_csv('context_1+r_2e.csv')\n","# brc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ms1ObHZxTYSI"},"source":["# br=pd.read_csv('response1.csv')\n","# br"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqzLS7rHTp4T"},"source":["# bc=pd.read_csv('contexto.csv')\n","# bc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpX1uTwjUPY6"},"source":["# bertc=np.array(bc[['0','1']])\n","# bertc1=np.multiply(bertc,0.1)\n","\n","# bertrcc=np.array(brcc[['0','1']])\n","# bertrcc1=np.multiply(bertrcc,0.1)\n","\n","# bertcr=np.array(brc[['0','1']])\n","# bertcr1=np.multiply(bertcr,0.4)\n","\n","# bertr=np.array(br[['0','1']])\n","# bertr1=np.multiply(bertr,0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BY2P7H1W2Wo4"},"source":["# yhats = [bertc1,bertr1,bertcr1,bertrcc1]\n","# import numpy as np\n","# yhats = np.array(yhats)\n","\n","# # sum across ensemble members\n","# summed = np.sum(yhats, axis=0)\n","# # argmax across classes\n","# result = np.argmax(summed, axis=1)\n","\n","# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","# accuracy_score(result,test_y)"],"execution_count":null,"outputs":[]}]}