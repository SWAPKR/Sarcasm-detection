{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BOW-reddit","provenance":[{"file_id":"1Gx2JKof2xF2-Z_rEo6eCj38X1aAdKKzq","timestamp":1621715272096}],"authorship_tag":"ABX9TyNv4kPr/kwZCUEStUi/Sjao"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAbRISagMiNQ","executionInfo":{"status":"ok","timestamp":1621715637838,"user_tz":-330,"elapsed":48740,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}},"outputId":"3d3b95c0-7b3b-4769-c709-985eca19cf01"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-VRI-txMuKc","executionInfo":{"status":"ok","timestamp":1621715639953,"user_tz":-330,"elapsed":2122,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}},"outputId":"04a137a9-3ee2-436e-9fee-1db35aa5595d"},"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","import re\n","import string\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","nltk.download('stopwords') \n","nltk.download('wordnet')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"l4YT5UIqMwGL","executionInfo":{"status":"ok","timestamp":1621715654219,"user_tz":-330,"elapsed":1485,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}}},"source":["df = pd.read_json('/content/drive/My Drive/sarcasm/reddit/sarcasm_detection_shared_task_reddit_training.jsonl',lines=True)\n","df1 = pd.read_json('/content/drive/My Drive/sarcasm/reddit/sarcasm_detection_shared_task_reddit_testing.jsonl',lines=True)\n","#df1['response'] = df1['response'].apply(preprocess_text)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7Hhs4xlM5wH","executionInfo":{"status":"ok","timestamp":1621715884851,"user_tz":-330,"elapsed":609,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}}},"source":["def preprocess_text(text):\n","    # Tokenise words while ignoring punctuation\n","    text = re.sub(r\"@USER\",\"\", text)\n","    tokeniser = RegexpTokenizer(r'\\w+')\n","    tokens = tokeniser.tokenize(text)\n","    \n","    # Lowercase and lemmatise \n","    lemmatiser = WordNetLemmatizer()\n","    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n","    \n","    # Remove stopwords\n","    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n","    return ' '.join(keywords)\n","\n","def preprocess_text2(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # remove stopwords\n","                word not in string.punctuation):  # remove punctuation\n","            # tweets_clean.append(word)\n","            stem_word = stemmer.stem(word)  # stemming word\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean\n","import re\n","\n","\n","def clean_text(text):\n","    \"\"\"\n","    Applies some pre-processing on the given text.\n","\n","    Steps :\n","    - Removing HTML tags\n","    - Removing punctuation\n","    - Lowering text\n","    \"\"\"\n","    text = re.sub(r\"@USER\",\"\", text)\n","    # remove HTML tags\n","    text = re.sub(r'<.*?>', '', text)\n","    \n","    # remove the characters [\\], ['] and [\"]\n","    text = re.sub(r\"\\\\\", \"\", text)    \n","    text = re.sub(r\"\\'\", \"\", text)    \n","    text = re.sub(r\"\\\"\", \"\", text)    \n","    \n","    # convert text to lowercase\n","    text = text.strip().lower()\n","    \n","    # replace punctuation characters with spaces\n","    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n","    translate_dict = dict((c, \" \") for c in filters)\n","    translate_map = str.maketrans(translate_dict)\n","    text = text.translate(translate_map)\n","\n","    return text"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jq8C5IW6RuuB","executionInfo":{"status":"ok","timestamp":1621715884853,"user_tz":-330,"elapsed":6,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}},"outputId":"c3c4d922-9779-466a-b41b-f30de9a39b45"},"source":["print(preprocess_text(\"this is check oplease !!!! cdcnsdj$$V4\"))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["check oplease cdcnsdj v4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TnzkM3bKOE5b","executionInfo":{"status":"ok","timestamp":1621715890801,"user_tz":-330,"elapsed":432,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}}},"source":["from sklearn.utils import shuffle\n","X_train, Y_train = shuffle(df['response'],df['label'],random_state=13)\n","X_test, Y_test = df1['response'],df1['label']"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTru-QRcOKRH","executionInfo":{"status":"ok","timestamp":1621715908950,"user_tz":-330,"elapsed":10933,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}},"outputId":"a6f3565b-9a31-4b01-8260-465e71a060ca"},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.svm import LinearSVC\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(stop_words=\"english\",\n","                             preprocessor=preprocess_text)\n","training_features = vectorizer.fit_transform(X_train)    \n","test_features = vectorizer.transform(X_test)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWdPtGXHPAOd","executionInfo":{"status":"ok","timestamp":1621715929137,"user_tz":-330,"elapsed":9216,"user":{"displayName":"UDIT KALANI","photoUrl":"","userId":"14115979490963977655"}},"outputId":"bf8d1162-a4f4-48d0-fd85-2f43e9c2af63"},"source":["#all ml models\n","#!pip install nbsvm-sklearn\n","#from nbsvm import NBSVMClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from xgboost import XGBClassifier\n","\n","# first, initialize the classificators\n","#nbsvm = NBSVMClassifier()\n","tree= DecisionTreeClassifier(random_state=24) # using the random state for reproducibility\n","forest= RandomForestClassifier(random_state=24)\n","knn= KNeighborsClassifier()\n","svm= SVC(random_state=24)\n","xboost= XGBClassifier(random_state=24) \n","models= [tree, forest, knn, svm, xboost]\n","for model in models:\n","    model.fit(training_features, Y_train) # fit the model\n","    y_pred= model.predict(test_features) # then predict on the test set\n","    accuracy= accuracy_score(Y_test, y_pred) # this gives us how often the algorithm predicted correctly\n","    clf_report= classification_report(Y_test, y_pred) # with the report, we have a bigger picture, with precision and recall for each class\n","    print(f\"The accuracy of model {type(model).__name__} is {accuracy:.2f}\")\n","    print(clf_report)\n","    print(\"\\n\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["The accuracy of model DecisionTreeClassifier is 0.56\n","              precision    recall  f1-score   support\n","\n"," NOT_SARCASM       0.55      0.57      0.56       900\n","     SARCASM       0.56      0.54      0.55       900\n","\n","    accuracy                           0.56      1800\n","   macro avg       0.56      0.56      0.56      1800\n","weighted avg       0.56      0.56      0.56      1800\n","\n","\n","\n","The accuracy of model RandomForestClassifier is 0.56\n","              precision    recall  f1-score   support\n","\n"," NOT_SARCASM       0.56      0.54      0.55       900\n","     SARCASM       0.56      0.58      0.57       900\n","\n","    accuracy                           0.56      1800\n","   macro avg       0.56      0.56      0.56      1800\n","weighted avg       0.56      0.56      0.56      1800\n","\n","\n","\n","The accuracy of model KNeighborsClassifier is 0.50\n","              precision    recall  f1-score   support\n","\n"," NOT_SARCASM       0.51      0.29      0.37       900\n","     SARCASM       0.50      0.72      0.59       900\n","\n","    accuracy                           0.50      1800\n","   macro avg       0.50      0.50      0.48      1800\n","weighted avg       0.50      0.50      0.48      1800\n","\n","\n","\n","The accuracy of model SVC is 0.56\n","              precision    recall  f1-score   support\n","\n"," NOT_SARCASM       0.56      0.51      0.54       900\n","     SARCASM       0.55      0.60      0.58       900\n","\n","    accuracy                           0.56      1800\n","   macro avg       0.56      0.56      0.56      1800\n","weighted avg       0.56      0.56      0.56      1800\n","\n","\n","\n","The accuracy of model XGBClassifier is 0.54\n","              precision    recall  f1-score   support\n","\n"," NOT_SARCASM       0.53      0.86      0.65       900\n","     SARCASM       0.61      0.22      0.32       900\n","\n","    accuracy                           0.54      1800\n","   macro avg       0.57      0.54      0.49      1800\n","weighted avg       0.57      0.54      0.49      1800\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kuRnYBpLRf8p"},"source":[""],"execution_count":null,"outputs":[]}]}